{"cells": [{"cell_type": "code", "execution_count": 1, "id": "dc333c1d", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nspark=SparkSession.builder.appName('logisticregression').getOrCreate()"}, {"cell_type": "code", "execution_count": 19, "id": "278530ff", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Names: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Total_Purchase: double (nullable = true)\n |-- Account_Manager: integer (nullable = true)\n |-- Years: double (nullable = true)\n |-- Num_Sites: double (nullable = true)\n |-- Onboard_date: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Company: string (nullable = true)\n |-- Churn: integer (nullable = true)\n\n"}], "source": "customer_data = spark.read.csv('gs://bigdatabucket30/customer_churn.csv',inferSchema=True,header=True)\n\ncustomer_data.printSchema()"}, {"cell_type": "code", "execution_count": 20, "id": "e39dce75", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|       Onboard_date|            Location|             Company|              Churn|\n+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n|  count|          900|              900|              900|               900|              900|               900|                900|                 900|                 900|                900|\n|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|               null|                null|                null|0.16666666666666666|\n| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|               null|                null|                null| 0.3728852122772358|\n|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|2006-01-02 04:16:13|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|2016-12-28 04:07:38|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n\n"}], "source": "customer_data.describe().show()"}, {"cell_type": "code", "execution_count": 21, "id": "5eec4faa", "metadata": {}, "outputs": [{"data": {"text/plain": "['Names',\n 'Age',\n 'Total_Purchase',\n 'Account_Manager',\n 'Years',\n 'Num_Sites',\n 'Onboard_date',\n 'Location',\n 'Company',\n 'Churn']"}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": "customer_data.columns"}, {"cell_type": "code", "execution_count": 22, "id": "18769002", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[Names: string, Age: double, Total_Purchase: double, Account_Manager: int, Years: double, Num_Sites: double, Onboard_date: string, Location: string, Company: string, Churn: int, features: vector]"}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.ml.feature import VectorAssembler\nvector_assembler = VectorAssembler(inputCols=['Age','Total_Purchase','Account_Manager','Years','Num_Sites'],outputCol='features')\noutput = vector_assembler.transform(customer_data)\n\noutput"}, {"cell_type": "code", "execution_count": 23, "id": "cae3d2c0", "metadata": {}, "outputs": [], "source": "originaldata = output.select('features','churn')\ntraining_data,test_data = originaldata.randomSplit([0.8,0.2])"}, {"cell_type": "code", "execution_count": 35, "id": "565ef55c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+-------------------+-------------------+\n|summary|              churn|         prediction|\n+-------+-------------------+-------------------+\n|  count|                708|                708|\n|   mean| 0.1483050847457627|0.10310734463276836|\n| stddev|0.35565340410241986| 0.3043140170821189|\n|    min|                0.0|                0.0|\n|    max|                1.0|                1.0|\n+-------+-------------------+-------------------+\n\n"}], "source": "from pyspark.ml.classification import LogisticRegression\nlogistic_regression = LogisticRegression(labelCol='churn')\nmodel = logistic_regression.fit(training_data)\n\ntraining_summary = model.summary\ntraining_summary.predictions.describe().show()"}, {"cell_type": "code", "execution_count": 26, "id": "7d82dba3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-----+--------------------+--------------------+----------+\n|            features|churn|       rawPrediction|         probability|prediction|\n+--------------------+-----+--------------------+--------------------+----------+\n|[25.0,9672.03,0.0...|    0|[4.49480858114781...|[0.98895650277304...|       0.0|\n|[28.0,9090.43,1.0...|    0|[1.69619749840099...|[0.84503745357781...|       0.0|\n|[28.0,11245.38,0....|    0|[3.49941128083266...|[0.97067101376560...|       0.0|\n|[29.0,8688.17,1.0...|    1|[2.79425225888153...|[0.94236443557162...|       0.0|\n|[29.0,9617.59,0.0...|    0|[4.27445197497661...|[0.98627142237888...|       0.0|\n|[30.0,10744.14,1....|    1|[1.74693264366352...|[0.85156550087731...|       0.0|\n|[31.0,8688.21,0.0...|    0|[6.49262917259758...|[0.99848772870366...|       0.0|\n|[31.0,10058.87,1....|    0|[4.32613202563539...|[0.98695387338568...|       0.0|\n|[32.0,8617.98,1.0...|    1|[1.21111437751044...|[0.77049606591240...|       0.0|\n|[32.0,10716.75,0....|    0|[4.28614191735786...|[0.98642880895794...|       0.0|\n|[32.0,11715.72,0....|    0|[3.37063219108148...|[0.96677400441422...|       0.0|\n|[33.0,7720.61,1.0...|    0|[1.95346301944155...|[0.87582375813060...|       0.0|\n|[33.0,8556.73,0.0...|    0|[3.77672922495252...|[0.97761509595302...|       0.0|\n|[33.0,10309.71,1....|    0|[6.40937793293958...|[0.99835665688589...|       0.0|\n|[33.0,10709.39,1....|    0|[6.11005907880787...|[0.99778449971115...|       0.0|\n|[33.0,12249.96,0....|    0|[5.49273610720445...|[0.99590031124971...|       0.0|\n|[34.0,6461.86,1.0...|    0|[4.39812733244304...|[0.98784910744526...|       0.0|\n|[34.0,7324.32,0.0...|    0|[1.13700037743060...|[0.75712847908829...|       0.0|\n|[34.0,7818.13,0.0...|    0|[3.69447835457799...|[0.97574262977386...|       0.0|\n|[34.0,9845.35,0.0...|    0|[5.50969890348254...|[0.99596898897955...|       0.0|\n+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\nprediction = model.evaluate(test_data)\n\nprediction.predictions.show()"}, {"cell_type": "code", "execution_count": 27, "id": "d1816246", "metadata": {}, "outputs": [{"data": {"text/plain": "0.7675736961451247"}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": "binaryCE = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='churn')\nevaluation = binaryCE.evaluate(prediction.predictions)\nevaluation"}, {"cell_type": "code", "execution_count": 32, "id": "d94a22d4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Names: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Total_Purchase: double (nullable = true)\n |-- Account_Manager: integer (nullable = true)\n |-- Years: double (nullable = true)\n |-- Num_Sites: double (nullable = true)\n |-- Onboard_date: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Company: string (nullable = true)\n\n"}], "source": "new_logistic_regression = logistic_regression.fit(data)\nnew_customer_data = spark.read.csv('gs://bigdatabucket30/new_customers.csv',inferSchema=True,header=True)\n\nnew_customer_data.printSchema()"}, {"cell_type": "code", "execution_count": 33, "id": "5506b1dd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Names: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Total_Purchase: double (nullable = true)\n |-- Account_Manager: integer (nullable = true)\n |-- Years: double (nullable = true)\n |-- Num_Sites: double (nullable = true)\n |-- Onboard_date: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Company: string (nullable = true)\n |-- features: vector (nullable = true)\n\n"}], "source": "new_test_data = vector_assembler.transform(new_customer_data)\n\nnew_test_data.printSchema()"}, {"cell_type": "code", "execution_count": 36, "id": "5ef281a7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+----------+\n|         Company|prediction|\n+----------------+----------+\n|        King Ltd|       0.0|\n|   Cannon-Benson|       1.0|\n|Barron-Robertson|       1.0|\n|   Sexton-Golden|       1.0|\n|        Wood LLC|       0.0|\n|   Parks-Robbins|       1.0|\n+----------------+----------+\n\n"}], "source": "final_output = new_logistic_regression.transform(new_test_data)\n\nfinal_output.select('Company','prediction').show()"}, {"cell_type": "code", "execution_count": null, "id": "69ad1abc", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}